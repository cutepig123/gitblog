# [測試onnx directml](https://github.com/cutepig123/gitblog/issues/82)

測試onnx directml

安裝指引
https://onnxruntime.ai/docs/genai/tutorials/phi3-v.html
```
pip install huggingface-hub[cli]
huggingface-cli download microsoft/Phi-3.5-vision-instruct-onnx --include gpu/gpu-int4-rtn-block-32/* --local-dir .
pip install onnxruntime-genai-directml
curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3v.py -o phi3v.py
pip install pyreadline3
python phi3v.py -m gpu/gpu-int4-rtn-block-32 -p dml
```

結果：顯存不夠，死了

使用cpu計算
```
(onnx-py312) D:\codes\onnx>python phi3v.py -m gpu/gpu-int4-rtn-block-32 -p cpu
Loading model...
Image Path (comma separated; leave empty if no image): table.png
['table.png']
Loading images...
Prompt: Convert this image to markdown format
Processing images and prompt...
Generating response...
等了半天，報錯
2024-11-16 19:44:52.3070249 [E:onnxruntime:onnxruntime-genai, sequential_executor.cc:516 onnxruntime::ExecuteKernel] Non-zero status code returned while running SkipSimplifiedLayerNormalization node. Name:'/model/layers.0/post_attention_layernorm/SkipLayerNorm' Status Message: D:\a\_work\1\s\include\onnxruntime\core/framework/op_kernel_context.h:42 onnxruntime::OpKernelContext::Input Missing Input: model.layers.0.post_attention_layernorm.weight

Traceback (most recent call last):
  File "D:\codes\onnx\phi3v.py", line 87, in <module>
    run(args)
  File "D:\codes\onnx\phi3v.py", line 65, in run
    generator.compute_logits()
onnxruntime_genai.onnxruntime_genai.OrtException: Non-zero status code returned while running SkipSimplifiedLayerNormalization node. Name:'/model/layers.0/post_attention_layernorm/SkipLayerNorm' Status Message: D:\a\_work\1\s\include\onnxruntime\core/framework/op_kernel_context.h:42 onnxruntime::OpKernelContext::Input Missing Input: model.layers.0.post_attention_layernorm.weight
```

小模型測試
https://github.com/microsoft/onnxruntime-genai?tab=readme-ov-file
```
huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir .
pip install numpy
pip install --pre onnxruntime-genai
import onnxruntime_genai as og

model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()
 
# Set the max length to something sensible by default,
# since otherwise it will be set to the entire context length
search_options = {}
search_options['max_length'] = 2048

chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

text = input("Input: ")
if not text:
   print("Error, input cannot be empty")
   exit

prompt = f'{chat_template.format(input=text)}'

input_tokens = tokenizer.encode(prompt)

params = og.GeneratorParams(model)
params.set_search_options(**search_options)
params.input_ids = input_tokens
generator = og.Generator(model, params)

print("Output: ", end='', flush=True)

try:
   while not generator.is_done():
     generator.compute_logits()
     generator.generate_next_token()

     new_token = generator.get_next_tokens()[0]
     print(tokenizer_stream.decode(new_token), end='', flush=True)
except KeyboardInterrupt:
    print("  --control+c pressed, aborting generation--")

print()
del generator
```

運行成功不過是用cpu

Q:如何知道huggingface能下載的模型有哪些？
Q；上面那個如何改成gpu？

